{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(r\"../input/titanic/train.csv\")\ntest_data = pd.read_csv(r\"../input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\ndef nan_padding(data, columns):\n    for column in columns:\n        imputer = SimpleImputer()\n        data[column] = imputer.fit_transform(data[column].values.reshape(-1,1))\n    return data\n\nnan_columns = [\"Age\", \"SibSp\", \"Parch\"]\n\ntrain_data = nan_padding(train_data, nan_columns)\ntest_data = nan_padding(test_data, nan_columns)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_passenger_id=test_data[\"PassengerId\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_not_concerned(data, columns):\n    return data.drop(columns, axis=1)\n\nnot_concerned_columns = [\"PassengerId\", \"Name\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]\ntrain_data = drop_not_concerned(train_data, not_concerned_columns)\ntest_data = drop_not_concerned(test_data, not_concerned_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dummy_data(data, columns):\n    for column in columns:\n        data = pd.concat([data, pd.get_dummies(data[column], prefix=column)], axis=1)\n        data = data.drop(column, axis=1)\n    return data\n\ndummy_columns = [\"Pclass\"]\ntrain_data = dummy_data(train_data, dummy_columns)\ntest_data= dummy_data(test_data, dummy_columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndef sex_to_int(data):\n    le = LabelEncoder()\n    le.fit([\"male\", \"female\"])\n    data[\"Sex\"]=le.transform(data[\"Sex\"])\n    return data\n\ntrain_data = sex_to_int(train_data)\ntest_data = sex_to_int(test_data)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndef normalize_age(data):\n    scalar = MinMaxScaler()\n    data[\"Age\"] = scalar.fit_transform(data[\"Age\"].values.reshape(-1,1))\n    return data\n\ntrain_data = normalize_age(train_data)\ntest_data = normalize_age(test_data)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\ndef split_valid_test_data(data, fraction=(1 - 0.8)):\n    data_y = data[\"Survived\"]\n    lb = LabelBinarizer()\n    \n    data_y = lb.fit_transform(data_y)    \n    data_x = data.drop([\"Survived\"], axis=1)\n    \n    train_x, valid_x, train_y, valid_y = train_test_split(data_x, data_y, test_size=fraction)\n    return train_x.values, train_y, valid_x, valid_y\n\ntrain_x, train_y, valid_x, valid_y = split_valid_test_data(train_data)\n\nprint(\"train_x:{}\".format(train_x.shape))\nprint(\"train_y:{}\".format(train_y.shape))\nprint(\"train_y content:{}\".format(train_y[:3]))\n\nprint(\"valid_x:{}\".format(valid_x.shape))\nprint(\"valid_y:{}\".format(valid_y.shape))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from collections import namedtuple\nfrom keras.utils import to_categorical\nfrom sklearn.preprocessing import Binarizer\n\ndef build_neural_network(hidden_units=10):\n    #tf.reset_default_graph()\n    # inputs = tf.Variable(shape=[None, train_x.shape[1]], name=\"input\")\n    # print(train_x.shape[1])\n    #inputs = tf.placeholder(tf.float32, shape=[None, train_x.shape[1]])\n    #labels = tf.placeholder(tf.float32, shape=[None, 1])\n    #learning_rate = tf.placeholder(tf.float32)\n    #is_training=tf.Variable(True,dtype=tf.bool)\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(hidden_units, input_shape=(train_x.shape[1],), kernel_initializer=\"glorot_uniform\", activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(1, activation='sigmoid'),\n    ])\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n    model.fit(train_x, train_y, epochs=50)\n        \n    loss_and_metrics = model.evaluate(valid_x, valid_y, batch_size=100)\n    print(loss_and_metrics)\n\n    \n    y_data = model.predict(test_data, batch_size=1)\n    # print(y_data)\n    \n    \n    binarizer=Binarizer(0.5)\n    test_predict_result=binarizer.fit_transform(y_data)\n    test_predict_result=test_predict_result.astype(np.int32)\n    # test_predict_result[:10]\n    \n    passenger_id=test_passenger_id.copy()\n    evaluation=passenger_id.to_frame()\n    evaluation[\"Survived\"] = test_predict_result\n    # evaluation[:10]\n    \n\n    # print(evaluation)\n    evaluation.to_csv(\"evaluation_submission.csv\", index=False)\n\n    \nbuild_neural_network(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_barch(data_x, data_y, batch_size=32):\n    batch_n=len(data_x)//batch_size\n    for i in (batch_n):\n        batch_x=data_x[i*batch_size:(i+1)*batch_size]\n        batch_y=data_y[i*batch_size:(i+1)*batch_size]\n        yield batch_x,batch_y\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}